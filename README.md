# ğŸ–ï¸ Hand Gesture Virtual Mouse & Keyboard

Control your computer using just your hands âœ‹ğŸ–±ï¸âŒ¨ï¸.
This project uses **MediaPipe** and **OpenCV** to track hand landmarks, combined with **pynput** and **pyautogui** to simulate real mouse and keyboard actions.

It includes:

* **Virtual Mouse** â€“ Move the pointer, click, and scroll with simple pinch gestures.
* **Virtual Keyboard** â€“ Type letters, space, backspace, and enter by hovering your fingertip over an on-screen keyboard.

---

## âœ¨ Features

### ğŸ–±ï¸ Virtual Mouse

* Move the mouse using your **palm center**.
* Smooth cursor movement with built-in filtering.
* Perform common actions with **pinch gestures**:

  * ğŸ‘‰ **Thumb + Index** â†’ Left Click
  * ğŸ‘‰ **Thumb + Middle** â†’ Right Click
  * ğŸ‘‰ **Thumb + Ring** â†’ Scroll Up
  * ğŸ‘‰ **Thumb + Pinky** â†’ Scroll Down

### âŒ¨ï¸ Virtual Keyboard

* On-screen keyboard layout (Aâ€“Z + Space, Backspace, Enter).
* **Hover your index fingertip** over a key to highlight it.
* Hold for **1 second** to â€œpressâ€ the key.
* Typed text is displayed on screen **and** entered into the system (via `pynput`).

---

## âš™ï¸ Requirements

* Python **3.8+**
* Install dependencies with:

```bash
pip install -r requirements.txt
```

`requirements.txt` contains:

```
opencv-python
mediapipe
pynput
pyautogui
```

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ virtual_mouse.py       # Hand gesture-based mouse controller
â”œâ”€â”€ virtual_keyboard.py    # On-screen keyboard controlled by hand gestures
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md              # Documentation
```

---

## â–¶ï¸ Usage

### Run Virtual Mouse

```bash
python virtual_mouse.py
```

### Run Virtual Keyboard

```bash
python virtual_keyboard.py
```

â¡ï¸ Press **Q** anytime to quit.

---

## ğŸ”§ Tips

* Use in **good lighting** for accurate hand tracking.
* Works best with **one hand visible**.
* Adjust `pinch_threshold` (mouse) or **selection delay** (keyboard) if gestures feel too sensitive or slow.

---

## ğŸš€ Future Improvements

* âœ… Multi-hand support
* âœ… Extra mouse gestures (drag & drop, copy-paste)
* âœ… Smarter keyboard (auto-suggest / predictive typing)
* âœ… GUI overlay instead of drawn rectangles

---

## ğŸŒŸ Conclusion

This project shows how computer vision and hand-tracking can make humanâ€“computer interaction more natural.
Itâ€™s a simple start, but with more gestures, AI-powered text prediction, and multi-hand support, it can grow into a fully functional **touchless control system**.

If you like this project, â­ the repo and feel free to fork, improve, or share your own ideas! ğŸš€

---
